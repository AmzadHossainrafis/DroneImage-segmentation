{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "import os \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import glob \n",
    "import pandas as pd \n",
    "import albumentations as A\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\Amzad\\Desktop\\camvid-segmentation\\dataset\\DronData\\dataset\\semantic_drone_dataset'\n",
    "original_images_path = os.path.join(data_path, 'original_images/')\n",
    "label_images_path = os.path.join(data_path, 'label_images_semantic/')\n",
    "label_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with id of the images without extensions (.jpg)\n",
    "def create_df():\n",
    "    name = []\n",
    "    mask = []\n",
    "    for dirname, _, filenames in os.walk(original_images_path): # given a directory iterates over the files\n",
    "        for filename in filenames:\n",
    "            f = filename.split('.')[0]\n",
    "            name.append(f)\n",
    "\n",
    "    return pd.DataFrame({'id': name}, index = np.arange(0, len(name))).sort_values('id').reset_index(drop=True)\n",
    "\n",
    "\n",
    "df = create_df() \n",
    "\n",
    "X = create_df()['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val = train_test_split(X, test_size=0.25, random_state=123)\n",
    "X_test, X_val = train_test_split(X_val, test_size=0.4, random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Size   : ', len(X_train))\n",
    "print('Val Size     : ', len(X_val))\n",
    "print('Test Size    : ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, img_path, mask_path, X, transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.mask_path = mask_path\n",
    "        self.X = X\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.img_path + self.X[idx] + '.jpg'))\n",
    "        mask = np.array(Image.open(self.mask_path + self.X[idx] + '.png')) # relabel classes from 1,2 --> 0,1 where 0 is background\n",
    "        \n",
    "        # augment images\n",
    "        if self.transform!=None:\n",
    "            aug = self.transform(image=image, mask=mask)\n",
    "            image = aug['image']\n",
    "            mask = aug['mask']\n",
    "        \n",
    "        norm = A.Normalize()(image = image, mask = np.expand_dims(mask, 0))\n",
    "        #covert mask to one hot encoding \n",
    "        mask_one_hot = np.zeros(( mask.shape[0], mask.shape[1],22),)\n",
    "        for i, unique_value in enumerate(np.unique(mask)):\n",
    "            mask_one_hot[:, :, i][mask == unique_value] = 1\n",
    "\n",
    "\n",
    "        return norm['image'].transpose(2, 0, 1), mask_one_hot.transpose(2, 0, 1).astype('float32')\n",
    "        \n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = A.Compose([A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.GaussNoise(),\n",
    "             A.GridDistortion(p=0.2), A.RandomBrightnessContrast((0,0.5),(0,0.5), p=0.4),A.Resize(512, 512)])\n",
    "\n",
    "\n",
    "# t = A.Compose([A.Resize(512, 512)])\n",
    "t2= A.Compose([A.Resize(512,512)])\n",
    "\n",
    "# datasets\n",
    "train_dataset = DroneDataset(original_images_path, label_images_path, X_train,t)\n",
    "valid_dataset = DroneDataset(original_images_path, label_images_path, X_val,t)\n",
    "test_dataset = DroneDataset(original_images_path, label_images_path, X_test)\n",
    "\n",
    "# dataloader\n",
    "batch_size = 8\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dl = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "def diceloss(inputs=None, targets=None, smooth=1):\n",
    "    \n",
    "    #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "    #inputs = F.sigmoid(inputs)       \n",
    "    \n",
    "    #flatten label and prediction tensors\n",
    "    inputs = inputs.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    \n",
    "    intersection = (inputs * targets).sum()                            \n",
    "    dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "    \n",
    "    return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch.utils.metrics as mtrcs\n",
    "arch = 'unet'\n",
    "enc_name = 'efficientnet-b0'\n",
    "classes = 22\n",
    "\n",
    "iou = mtrcs.IoU(threshold=0.5, ignore_channels=None) \n",
    "f1 = mtrcs.Fscore(threshold=0.5, beta=1, ignore_channels=None)\n",
    "\n",
    "model = smp.create_model(arch,\n",
    "                         encoder_name = enc_name,\n",
    "                         encoder_weights = \"imagenet\",\n",
    "                         in_channels = 3,\n",
    "                         activation = 'softmax',\n",
    "                         classes = classes).to(\"cuda\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-03)\n",
    "#criterion = smp.losses.FocalLoss(\"multiclass\", alpha=0.5, gamma=2, ignore_channels=None, balance=None, reduction='mean')\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "#criterion = criterion(torch.from_numpy(y).to(\"cuda\"), torch.from_numpy(y).to(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "    smp.utils.metrics.Fscore(threshold=0.5),\n",
    "]\n",
    "\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=\"cuda\",\n",
    "    verbose=\"cuda\",\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=\"cuda\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model for 40 epochs\n",
    "\n",
    "max_score = 0\n",
    "for i in range(0, 40):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_dl)\n",
    "    valid_logs = valid_epoch.run(val_dl)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')\n",
    "        \n",
    "    if i == 25:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "        print('Decrease decoder learning rate to 1e-5!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y \n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class SoftDiceLossV1(nn.Module):\n",
    "    '''\n",
    "    soft-dice loss, useful in binary segmentation\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 p=1,\n",
    "                 smooth=1):\n",
    "        super(SoftDiceLossV1, self).__init__()\n",
    "        self.p = p\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        '''\n",
    "        inputs:\n",
    "            logits: tensor of shape (N, H, W, ...)\n",
    "            label: tensor of shape(N, H, W, ...)\n",
    "        output:\n",
    "            loss: tensor of shape(1, )\n",
    "        '''\n",
    "        probs = torch.sigmoid(logits)\n",
    "        numer = (probs * labels).sum()\n",
    "        denor = (probs.pow(self.p) + labels.pow(self.p)).sum()\n",
    "        loss = 1. - (2 * numer + self.smooth) / (denor + self.smooth)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IOU(nn.Module): \n",
    "    def __init__(self, n_classes=22):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes \n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        #intersection between pred and target at dimention -1 \n",
    "        intersection = np.logical_and(target, pred)\n",
    "        #union between pred and target at dimention -1\n",
    "        union = np.logical_or(target, pred)\n",
    "        #sum of intersection and union\n",
    "        iou_score = np.sum(intersection) / np.sum(union)\n",
    "        return iou_score \n",
    "    \n",
    "    def __str__(self): \n",
    "        return f\"IOU(n_classes={self.n_classes})\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tqdm\n",
    "from tqdm import tqdm\n",
    "def train(model, train_dl, val_dl, optimizer, criterion, epochs=10):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    dice_score = []\n",
    "    iou_score = []\n",
    "\n",
    "    print(\"Training Started...\")\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        #use tqdm for progress bar \n",
    "        for x, y in train_dl: \n",
    "            x = x.to(\"cuda\")\n",
    "            y = y.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            #soft dice loss \n",
    "       \n",
    "         \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / len(train_dl)\n",
    "        train_loss.append(epoch_loss)\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        for x, y in val_dl:\n",
    "            x = x.to(\"cuda\")\n",
    "            y = y.to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(x)\n",
    "                iou_score.append(IOU()(y_hat.cpu().numpy(), y.cpu().numpy()))\n",
    "                \n",
    "                \n",
    "                loss = criterion(y_hat, y)\n",
    "                running_loss += loss.item()\n",
    "        epoch_loss = running_loss / len(val_dl)\n",
    "        val_loss.append(epoch_loss)\n",
    "        print(f\"Val Loss: {epoch_loss:.4f}\")\n",
    "        print(f\"IoU Score: {np.mean(iou_score):.4f}\")\n",
    "   \n",
    "        \n",
    "\n",
    "    return train_loss, val_loss, dice_score, iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softd= SoftDiceLossV1()\n",
    "\n",
    "train_loss, val_loss = train(model, train_dl, val_dl, optimizer, softd, epochs=10)\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'model_10-30-23.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_to_rgb(one_hot_tensor):\n",
    "    \"\"\"\n",
    "    Converts a one-hot encoded tensor to an rgb image\n",
    "    \"\"\"\n",
    "    # First, convert to an rgb image with channels = classes\n",
    "    single_channel = torch.argmax(one_hot_tensor, dim=1).squeeze(0)\n",
    "    # Next, create an empty tensor to hold the new image\n",
    "    rgb_tensor = torch.zeros((3, single_channel.shape[0], single_channel.shape[1]))\n",
    "    # Now, loop through and create the new image\n",
    "    for class_idx in range(len(class_dict)):\n",
    "        mask = single_channel == class_idx\n",
    "        rgb_tensor[0][mask] = torch.tensor(class_dict[class_idx][0])\n",
    "        rgb_tensor[1][mask] = torch.tensor(class_dict[class_idx][1])\n",
    "        rgb_tensor[2][mask] = torch.tensor(class_dict[class_idx][2])\n",
    "    return rgb_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss vs epochs\n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(val_loss, label='val loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss') \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch.utils.metrics as mtrcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = 'unet'\n",
    "enc_name = 'efficientnet-b0'\n",
    "classes = 22\n",
    "\n",
    "iou = mtrcs.IoU(threshold=0.5, ignore_channels=None) \n",
    "f1 = mtrcs.Fscore(threshold=0.5, beta=1, ignore_channels=None)\n",
    "\n",
    "model = smp.create_model(arch,\n",
    "                         encoder_name = enc_name,\n",
    "                         encoder_weights = \"imagenet\",\n",
    "                         in_channels = 3,\n",
    "                         activation = 'softmax',\n",
    "                         classes = classes).to(\"cuda\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(r'C:\\Users\\Amzad\\Desktop\\camvid-segmentation\\notebook\\best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on valset \n",
    "for x, y in val_dl: \n",
    "    #model.eval()\n",
    "    x = x.to(\"cuda\")\n",
    "    y = y.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(x)\n",
    "        #subplot(1,2,1)\n",
    "        plt.imshow(x[7].permute(1, 2, 0).cpu().numpy())\n",
    "        plt.show()\n",
    "        #subplot(1,2,2)\n",
    "        plt.imshow(np.argmax(y_hat[7].cpu().numpy(), axis=0))\n",
    "        plt.show()\n",
    "        plt.imshow(np.argmax(y[7].cpu().numpy(), axis=0))\n",
    "        plt.show()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image  = Image.open(r'C:\\Users\\Amzad\\Desktop\\camvid-segmentation\\dataset\\DronData\\dataset\\semantic_drone_dataset\\label_images_semantic\\001.png')\n",
    "single_image = np.array(single_image)\n",
    "print(single_image.shape)\n",
    "print(np.unique(single_image))\n",
    "one_hot=np.zeros((3,3,22))\n",
    "for i, unique_value in enumerate(np.unique(single_image)):\n",
    "    one_hot[:, :, i][single_image == unique_value] = 1\n",
    "print(one_hot.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
